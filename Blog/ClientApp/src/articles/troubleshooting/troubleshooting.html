<!DOCTYPE html>
<html>
<head>
    <title>Go big and get lucky</title>
</head>
<body>

    <article>
<p>Those who have played the game DOOM will recognize the BFG9000 as an enormous weapon that delivers a highly destructive round.  In the war on software defects there is no single analogy for the BFG9000.  There are, however, a variety of &quot;big gun&quot; techniques for dispatching bugs and defects.  Almost all of these techniques center around identifying and fixing root cause issues.  Fixing a root cause issue can have a beneficial impact as a result of the fix itself, or, as is often the case, it will bring to light other deficiencies that exist as a result of the root cause.  In either case, fixing a root cause issue can deliver a massively beneficial result - often with relatively little effort or risk.</p>
<h2 id="go-big-and-get-lucky">Go big and get lucky</h2>
<p>From a purely statistical standpoint, hunting down root cause issues puts the odds on your side.  The <a href="https://www.investopedia.com/terms/p/paretoprinciple.asp">Pareto Principle</a> states that eighty percent of consequences come from twenty percent of causes.  Pareto's principal is highly applicable in the domain of troubleshooting software defects.  This is good news for most software developers because it means that for a given application, correcting a few critical deficiencies will result in a cascade of beneficial results.</p>
<h2 id="where-to-find-trouble">Where to find trouble</h2>
<p>Not every application is a target for big gun troubleshooting.  Prime candidates tend to have these characteristics:</p>
<ul>
<li>Large code base, lots of business logic</li>
<li>Multiple integrated platforms such as order management, CRM, accounting, etc.</li>
<li>Many domain entities</li>
<li>Many ETL processes.</li>
<li>Large number of reports.</li>
<li>Older application, subject of numerous and/or ongoing improvements.</li>
</ul>
<p>Symptoms the application is distressed include:</p>
<ul>
<li>Slow execution</li>
<li>Incorrect data</li>
<li>Constantly encounters exceptions</li>
<li>Unusually complex or verbose code</li>
<li>Fragile code i.e. implementing minor changes causes breakage elsewhere in the application.</li>
<li>Difficult to work on - small improvements are often blocked by deficiencies in code that is only remotely related.</li>
</ul>
<p>As a contractor or new employee you may be tasked with stabilizing an application with the characteristics and symptoms described above.  It is possible you will have not have a good understanding of the business domain and certainly you will not have a full understanding of the more complex business logic.  You will probably be asking yourself &quot;How do I solve this problem?  Where do I begin?&quot;</p>
<p>If you walk around the company and talk to the developers who wrote the code you might hear some of them say things like &quot;We need to implement SOLID design principals&quot;.  Others might say &quot;We should create Microservices&quot;.  You might also hear &quot;We need to write testable code and unit tests.&quot;  All of these suggestions will most likely be true to some to extent.  However, all of these objectives are a step removed from your immediate goal which is to stabilize the code base and assure the company can use the program for their immediate day-to-day needs.  To get to the heart of the matter you need to first target two primary enemies: <em>Garbage data and redundant code</em>.</p>
<p>To be clear, re-architecting, implementing SOLID principals, and writing unit/integration tests are great ways to clean up and renew an application. Your <strong>first pass</strong> however, should be focused on closing the door on bad data and removing redundant code.  These types of deficiencies are almost always root causes and pointers to where further attention should be directed.</p>
<h2 id="know-your-enemy">Know your enemy</h2>
<h4 id="garbage-in-garbage-out">Garbage in, garbage out</h4>
<p>Garbage in, garbage out (GIGO) is one of the oldest axioms in computing.  By preventing garbage data from ever entering your database you can avoid validating it again and again throughout application layers.</p>
<p>Sources of garbage data include:</p>
<ul>
<li>Improper database schema - wrongly nullable fields, wrongly typed fields, missing PK's, FK's, and unique constraints.</li>
<li>Duplicate data.</li>
<li>Multiple entry points for data.</li>
<li>Swallowing exceptions.</li>
<li>Missing validation rules.</li>
<li>Not programming defensively - making assumptions on behalf of the user, open ended if and case statements.</li>
</ul>
<h4 id="dont-repeat-yourself">Don't repeat yourself</h4>
<p>Don't Repeat Yourself (DRY) is the mother of all architectural principals.  Redundant code is caused by failing to factor code correctly and/or failing to place code in the correct application layer.  Redundant code is often found in:</p>
<ul>
<li>Controllers</li>
<li>WCF .svc files</li>
<li>Stored procedures</li>
<li>Report definition files</li>
</ul>
<p>It is worth noting that it is possible to build a large enterprise application with no knowledge or regard for SOLID principals.  There are thousands of such applications in use today.  Building an enterprise application with no regard for DRY is much more difficult because the application begins to work against itself almost immediately.  The size of the code base begins to grow exponentially as the application grows and it soon becomes impossible to scale.</p>
<h2 id="check-your-database-schema">Check your database schema</h2>
<p>Your first mission is to analyze your database schema - not from the perspective of a DBA, but as a user of the application.  You have a single objective: to verify the schema implements every possible safeguard to ensure data is always in a valid state.</p>
<div style="background-color:lightblue;padding:12px;margin:10px;">
Tip: If your database platform is Microsoft SQL Server, you can generate a script of your database.  In SSMS, right click on the database and click Tasks, than click Generate Scripts.  Choose all database objects and save the script to a single file.  You can use this file in several of the exercises in this section.  
</div>
<h4 id="check-for-invalid-data-types-primary-keys-foreign-keys-and-unique-constraints">Check for invalid data types, primary keys, foreign keys, and unique constraints</h4>
<p>This may seem obvious but you should be using primary keys and foreign keys correctly.  If your primary key does not capture the unique characteristics of a row consider using a unique constraint.  Scan each column in each table and make sure the data type is correct.  The constraint checks done by your DBMS are a powerful weapon against a wide variety bugs.  Keep your DBMS working for you to the greatest extent possible at all times.</p>
<h4 id="check-for-improperly-nullable-columns">Check for improperly nullable columns</h4>
<p>This is one of the most important exorcises you can undertake to repair a broken or poorly maintained schema.  If your application suffers from data integrity problems you should plan to spend some time here.  The goal is to check each column that allows a null value and determine if the column should actually be nullable.</p>
<p>Nullable columns should be avoided if at all possible.  One of the problems with nullable columns is that they bypass required value checking done by the DBMS.  As mentioned previously you want to keep the DBMS working for you at all times.  With that said, there is nothing inherently wrong with nullable columns - certain data elements are not always mandatory and thats just the way it is.</p>
<p>If you find a nullable column in your database, check to see if the column is used in the WHERE, GROUP BY, or ORDER BY clause of any query.  If it is, that is a good sign that the column should not be nullable.</p>
<p>When you check nullable columns on your database you get some very valuable information as a free byproduct of your analysis.  While this information may not be directly tied to the validity of your data you should take notes for future work you may want to do.  If you conclude a column MUST be nullable you should look carefully at the reasoning behind your conclusion as it may be a sign that you can better normalize or restructure your database.  This is best explained with an illustration.  Suppose you have a table called ORDERS that has a schema that looks like this:</p>
<pre><code>[ID] [int] IDENTITY(1,1) NOT NULL,
[CustomerID] [int] NOT NULL,
[OrderDate] [datetime2](0) NOT NULL,
[ShipDate] [datetime2](0) NULL,
[OrderAmount] [decimal](30, 6) NOT NULL,
[Notes] [nvarchar](3000) NULL,
// More columns...
</code></pre>
<p>In the table above the ShipDate column is nullable.  An argument can be made that the column should be nullable because at the time an order is placed the ship date does not yet exist and cannot be known with certainty.  That argument is entirely correct and would seem to support the column being nullable.  But there is a deeper issue here, and that is that the ShipDate column does not belong on the ORDERS table.  It belongs on the SHIPMENTS table and it should not be nullable.  That is how you use a nullable column check to find a design error in your database.</p>
<div style="background-color:lightblue;padding:12px;margin:10px;">
Tip: If you created a script file of your database you can use <a href="https://notepad-plus-plus.org">Notepad++</a> to quickly find NULL columns.  Open your file in Notepad++ and press Ctrl+F. Paste this into the "Find what" box: <code>(\) NULL)|(\] NULL)</code>. Set Search Mode to Regualar expression, then click "Find All in Current Document".
</div>
<h2 id="clean-up-your-data">Clean up your data</h2>
<h4 id="guard-the-perimiter">Guard the perimiter</h4>
<p>As a developer your ongoing objective is to make sure your database has valid, current, and complete data at all times.  One of the best ways to do this is to establish a single point of entry for each of your domain entities and enforce your validation rules at that point.  For example a method to insert or update a Customer object may look like this:</p>
<pre><code>// This is the only method that inserts or updates the customer table.
public string SaveCustomer(Customer customer)
{
  if(customer == null)
    throw new ArgumentNullException(&quot;customer&quot;);

  // all incoming Customer objects are validated here
  string errorMessage = ValidateCustomer(customer);

  if(! String.IsNullOrEmpty(errorMessage))
    return errorMessage;
  
  if(customer.ID == 0)
    // Insert...
  else
    // Update...

  return errorMessage;
}
</code></pre>
<p>Some developers will encounter edge cases where maintaining perfect data at all times will not be achievable or desirable.  Maintaining a pristine database is much like maintaining a highly normalized database.  Normalization is a worthy endeavor however it eventually reaches a point where usability and performance begin to diminish.  In the same sense, a hyper-vigilant approach to data cleanliness may at some point begin to conflict with the objectives of the business.  If this happens and you decide to allow your database to become less than pristine you will want to make sure the bar for doing so is very high.  Once you start mixing bad data with good you find yourself in the position of &quot;guarding from within&quot; which means your internal business logic becomes responsible for separating clean data from dirty.</p>
<p>A full discussion of how to design and maintain a database is beyond the scope of this article.  However there are a couple of points you can take away from this section:</p>
<ul>
<li>Be aware of your priorities and objectives in terms of maintaining a clean database.</li>
<li>Defend your objectives relentlessly.  Explore every option before making any concession to allow dirty data or partial data into your database.</li>
<li>Spend a lot of design time in this area.</li>
<li>The cost of getting it wrong here is high.</li>
</ul>
<h2 id="finding-the-enemy-in-code">Finding the enemy in code</h2>
<h4 id="get-out-of-your-own-way-organize-your-business-logic">Get out of your own way - organize your business logic</h4>
<p>Loosely defined, business logic is any code that depends on or operates on a domain entity such as an Order, Client, Product, etc.  Business logic belongs in one place only - the business logic layer.  Business logic libraries are .dll files that can be referenced by higher level layers or wrapped by some transport-specific layer such as REST or WCF.</p>
<p>Factoring business logic out of the presentation layer and into the business logic layer is an essential first step if you want to scale your application.  Business logic that is incorrectly placed in the presentation layer - such as a controller or view model -  is unusable to other controllers or view models that may need to call it.  The way to mitigate that obstacle is to move business logic to the lower level business logic layer where is accessible to all presentation objects.</p>
<p>It is not uncommon to find seventy percent or more of your application's code in the business logic layer.  Controllers and view models should be very thin and serve little purpose other than to map presentation functions to the business logic layer.</p>
<p>If you have a large and complex business logic layer consider using a facade or a variation thereof.  See <a href="https://github.com/leaderanalytics/AdaptiveClient">AdaptiveClient</a> for examples.</p>
<h4 id="dont-juggle-a-live-grenade-if-your-app-throws-stop-processing">Don't juggle a live grenade - if your app throws stop processing</h4>
<p>One of the most flagrant and fragrant violations found in many code bases today is incorrect error handling.  The problem is rooted in a thought process called &quot;Log and continue&quot; that holds dear three deeply flawed beliefs:</p>
<ul>
<li>More <code>try</code> / <code>catch</code> blocks are better.</li>
<li>Exceptions should be logged and execution should continue.</li>
<li>Exceptions should not be thrown when internal low-level code encounters unexpected or invalid business logic.</li>
</ul>
<p>These falsehoods, individually and collectively, are directly responsible for a multitude of afflictions which include:</p>
<ul>
<li>Errors that are difficult or literally impossible to trace.</li>
<li>Corrupt data.</li>
<li>Bloated, complex, and unnecessary error handling routines.</li>
<li>Propagation of ambiguous and/or contradictory logic.</li>
</ul>
<p>The correct approach to error handling is embodied in a pattern called Fail Fast which proposes these guidelines:</p>
<ul>
<li>Fewer <code>try</code>/<code>catch</code> blocks are better.  Exceptions should flow up to a global exception handler.</li>
<li><code>try</code>/<code>catch</code> blocks should only be used to catch specific errors that are actually handled in some way.</li>
<li>Unhandled Exceptions should be logged and execution should stop.</li>
<li>An Exception should be thrown immediately when internal low-level code encounters unexpected or invalid business logic.</li>
</ul>
<p>With regard to the last point above - don't be afraid to throw Exceptions when there is an EXPECTATION OF COMPLIANCE in your code.  Consider the following two methods:</p>
<pre><code>public Customer GetCustomerByID(int id)
{
    Customer c db.Customers.FirstOrDefault(x =&gt; x.ID == id);
}
</code></pre>
<p>In the code above you should not throw if the passed id is not found.  This is a public method.  There is no expectation the user knows a valid ID from an invalid one.</p>
<pre><code>private void ProcessOrder(Order order)
{
  Customer c = GetCustomerByID(order.CustomerID);

  if(c == null)
    throw new Exception($&quot;Invalid customer ID: {order.CustomerID}&quot;); 

  c.YTDOrders += order.Amount;
}
</code></pre>
<p>In the code above there is an expectation the customer ID on the order is correct.  Definitely throw if <code>order.CustomerID</code> is not found!  If an upstream change causes order.CustomerID to become invalid this code will likely catch that error in QA.</p>
<p>The code shown below is the wrong way to do it.  This error is common because developers are afraid to throw exceptions.  If an upstream change causes order.CustomerID to become invalid the error will most likely be discovered by the end user - after data corruption has occurred.</p>
<pre><code>private void ProcessOrder(Order order)
{
  Customer c = GetCustomerByID(order.CustomerID);

  // Horror
  if(c != null)
    c.YTDOrders += order.Amount;
  else
    Log.Error($&quot;Invalid customer ID: {order.CustomerID}&quot;); 
}
</code></pre>
<h4 id="use-the-compiler-luke">Use the compiler, Luke</h4>
<p>If there is any single analogy for the BFG9000 in computing MSBUILD.exe would surely be it.  Your compiler is the most powerful weapon you have.  Very technically speaking, failing to use the compiler effectively is probably not tied directly to garbage data or redundant code.  However the compiler is such an effective weapon that this article would not be complete if it did not exhort you to use it to your fullest advantage.  Early in your troubleshooting project you should check for and fix any code that passes or expects as a parameter an object of type <code>object</code>.  Avoid the <code>dynamic</code> keyword, and avoid using JavaScript. Except for the presentation layer.</p>
</article>


</body>
</html>